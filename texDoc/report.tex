\documentclass[11pt,fleqn]{article}
%\usepackage{CJK}
\usepackage{latexsym}
\usepackage{color}
\usepackage{graphicx, float}\usepackage{graphicx}
%\usepackage{algorithmicx}
\usepackage{algorithm}
\usepackage{algpseudocode}
%\usepackage[colorlinks]{hyperref}
\setlength{\oddsidemargin}{-0.0in}
\setlength{\evensidemargin}{-0.0in} \setlength{\textwidth}{6.0in}
\setlength{\textheight}{9.0in} \setlength{\topmargin}{-0.2in}

%\setlength{\leftmargin}{0.7in}
\usepackage{amssymb, graphicx, amsmath}  %  fancyheadings,
\usepackage{setspace}
\newcommand\qed{\qquad $\square$}
\newcommand{\nn}{\nonumber}

\def \[{\begin{equation}}
\def \]{\end{equation}}
\def\proof{{\bf Proof:\quad}}
\def \endzm {\quad $\Box$}
\def\dist{\hbox{dist}}


\newcommand{\R}{\mathbb{R}}
%\newtheorem{yinli}{ТэАн}[section]
\newcommand{\D}{\displaystyle}
\newcommand{\T}{\textstyle}
\newcommand{\SC}{\scriptstyle}
\newcommand{\FT}{\footnotesize}



%\newtheorem{theorem}{Theorem}[section]
%\renewcommand{\thetheorem}{\arabic{section}.\arabic{theorem}}
\newtheorem{definition}{Definition}
\renewcommand{\thedefinition}{\arabic{section}.\arabic{definition}}
\newtheorem{lemma}{Lemma}[section]
\renewcommand{\thelemma}{\arabic{section}.\arabic{lemma}}
\newtheorem{remark}{Remark}
\renewcommand{\theremark}{\arabic{section}.\arabic{remark}}
\newtheorem{proposition}{Proposition}[section]
\renewcommand{\theproposition}{\arabic{section}.\arabic{proposition}}
\newtheorem{corollary}{Corollary }[section]
\renewcommand{\thecorollary}{\arabic{section}.\arabic{corollary}}
\renewcommand{\theequation}{\arabic{section}.\arabic{equation}}
\renewcommand{\baselinestretch}{1.35}
\newtheorem{exam}{Example}[section]
\renewcommand{\theexam}{\arabic{section}.\arabic{exam}}
\newtheorem{theo}{Theorem}[section]
\renewcommand{\thetheo}{\arabic{section}.\arabic{theo}}
\begin{document}
%\begin{CJK*}{GBK}{song}

\begin{center}

{\LARGE \bf  Machine Learning Assignment 2}\\


\vskip 25pt
 {Huihuang Zheng, huihuang@utexas.edu }\\
\vskip 5pt
{\small hz4674 Spring 2016 }

\end{center}

\begin{spacing}{1.5}
\section{Using Code}
To run my code, open \emph{src/main.m}, change the \emph{DATA\_DIR} to where you put your MNIST .mat data, then run the \emph{src/main.m} with Matlab. It will output accuracy with different number of training images and different number of eigenvectors. In addition, my code will write original images and reconstruct images of few eigenvectors into \emph{src/out/} folder. My code was well commented so see them in details.

\section{What did I do}
I did experiments about how the number of training images and the number of eigenvectors we used can influence the results of PCA digits classification. I will say my steps in this section.
\subsection{From disk data file to matrix}
In file \emph{src/main.m}, I firstly read data from disk file (see \emph{src/readData.m}). Secondly, since the raw data is $28 * 28 * 1$ images and in the training set we have $60000$ images of hand writing digits. Due to the requirement of the assignment that training images should be less then data dimension, we should pick few images for training. I implemented two kinds of data picking (see \emph{src/pickData.m}), one is picking first $k$ data. The other is pick random $k$ data. Then, I convert these $28 * 28 * 1 * k$ 4-d data into $784 * k$ 2-d matrix $A$ (see \emph{src/imageFeature.m}). \\
\subsection{Get Eigenvectors and Mean Vector}
(See \emph{src/hw1FindEigendigits.m})Now, since every column of $A$ is a vector representing a image, it's easy to get mean image by just averaging columns. It's also easy to get eigenvalues and eigenvectors by calling Matlab function \emph{eig()}. Suppose $A$ is the matrix after subtracting mean with size $x * k$ where $x$ is the dimension of each data and $k$ is how many training data we picked. Because $k < x$, the formal way of computing eigenvalues and eigenvectors of $AA^T$, whose size is $x * x$, will be slower than computing eigenvalues and eigenvectors of $A^TA$, whose size is $k * k$. And any eigenvector $v$ and corresponding eigenvalue $u$ of $A^TA$, it satisfies that $A^TAv = uv$. Multiply both sides with $A$, we got $AA^T(Av) = u(Av)$, which means the $Av$ and $u$ are eigenvector and eigenvalue of $AA^T$. So in this assignment, I used this way to speed up computing eigenvalues and eigenvectors.
\subsection{Test}
After we get the eigenvectors and mean vector, we can predict what digit the test image is. I do prediction in the following way. First, I project both training images and testing images into eigenvector space, which means I pick largest $n$ eigenvectors. Those eigenvectors are in a matrix $V$ of size $x * n$. Suppose test images are reshaped as a matrix $B$ of size $x * k'$, then I subtract every column of $B$ with mean vector. After subtracting mean, let $P_B = B'V$. The $P_B$ is the project of $B$ with each row vector is the project of one testing image. Do same thing to training image and I get project of training data $P_A$. Second, I calculate Euclidean distance between every projected test data point and every projected training data point. For one test data, I predict the image has same digit label as the label of training image with minimum eigenvector-space projected Euclidean distance.

\section{Experiment Result}
I run experiment from 250 to 600 training data with an increment of 50. Used 10 to 80 eigenvectors with an increment of 10. In this section I will show my experiment result and discuss some results.\\
\\
Following figure shows the mean image I got from 600 training data. The training data has digits in about center of the image. \\
\begin{center}
  \includegraphics[width=0.5\linewidth]{mean.JPG}\\
  Fig.1 Mean Image
\end{center}

Following figure shows trend of accuracy with number of training images and number of eigenvectors.\\

\begin{center}
  \includegraphics[width=1.0\linewidth]{result.JPG}
  Fig.2 Accuracy Figure
\end{center}
Following table gives detail accuracy of above figure.
\begin{center}
  \includegraphics[width=1.0\linewidth]{table.JPG}
  Table 1. Detail Accuracy.
\end{center}
We can find even though accuracy increases with number of training data and number of eigenvalues, when the number of eigenvalues is high than 30, it cannot improve accuracy significantly. Using 80 eigenvectors even causes performance worse when with more than 400 training data. So I suggest with 30 eigenvalues can significantly predict what the digit is and with very few computation complexity (versus original dimension of 784 for each image).

Also, I reconstruct images from project space. I show reconstructed images from project space: left column is original image and right column is corresponding reconstruct image from eigenvector space.
\begin{center}
  \includegraphics[width=0.4\linewidth]{200_10_raw_1.JPG}
  \includegraphics[width=0.4\linewidth]{200_10_re_1.JPG}\\
  Fig 3. With 200 training data and 10 eigenvectors.
\end{center}

\begin{center}
  \includegraphics[width=0.4\linewidth]{200_40_raw_16.JPG}
  \includegraphics[width=0.4\linewidth]{200_40_re_16.JPG}\\
  Fig 4. With 200 training data and 40 eigenvectors.
\end{center}

\begin{center}
  \includegraphics[width=0.4\linewidth]{200_80_raw_3.JPG}
  \includegraphics[width=0.4\linewidth]{200_80_re_3.JPG}\\
  Fig 5. With 200 training data and 80 eigenvectors.
\end{center}

From these images, we can see in the eigenvector space, the image performs in different ways. It seems like blur in images due to add mean back. But we can see some pixels with high intensity and outline the digits.
\end{spacing}

%\end{CJK*}
\end{document}
